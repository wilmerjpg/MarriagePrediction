---
title: "Marriage Prediction Proyect"
author: "Yanelly Nobrega CI V-22785031 - Wilmer Prieto CI V-21468564"
date: "1 de mayo de 2016"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```



## Proyecto Predicción de Estado Civil 

Se tiene un dataset llamado marriageData el cual se debe analizar para detectar patrones y buscar relaciones entre sus atributos. Luego de realizar el análisis y usando dicho dataset, se aplicarán algoritmos de clasificacián, con la finalidad de generar un modelo que permita predecir el estado civil de una pareja usando las variables de marriageData que se consideren necesarias para ello.


##Se procede a instalar y cargar los paquetes necesarios

```{r, warning= FALSE}
#La instalación de los paquetes se muestra comentada porque ya se encuentran instalados en el ambiente de desarrollo
#install.packages('rpart.plot');
#install.packages('rpart');
#install.packages('pROC')
#install.packages('RWeka')
#install.packages('class')
library('rpart')
library('rpart.plot')
library('RWeka')
library('class')
library('pROC')

```

## Se almacena el archivo **marriageData.csv** como un dataset

Este archivo contiene los datos relacionados al estado civil de una pareja, los cuales serán utilizados para implementar y evaluar los modelos anteriormente mencionados, y de esta forma poder elegir el mejor modelo para la predicción que se desa realizar.

```{r, warning= FALSE}
marriageData <- read.csv("marriageData.csv")
```

```{r, warning= FALSE}
#Descripcion de la data
#ID = Identificador unico para cada registro(paerja) del dataset
#GAGE = Edad de la Mujer
#BAJE = Edad del Hombre
#GP = 
#BP =
#AINCOME = Sueldo Anual en dolares percibido por la pareja
```

## Preprocesamiento de la d  ata

Se realiza el preprocesamiento de la data, donde se eliminaron las columnas de ID y AINCOME, se realizó una numerizacion de atributos categóricos para las variables GP, BP Y STATUS. Y por último se creo una nueva columna que almacenará la diferencia de edad de la pareja, y una vez guardados los datos en esta columna se eliminaron las columnas BAGE y GAGE.

```{r, warning= FALSE}
marriageData$ID <- NULL
marriageData$AINCOME <- NULL

marriageData$GAGE <- as.numeric(marriageData$GAGE)
marriageData$BAGE <- as.numeric(marriageData$BAGE)

marriageData$GP <- as.character(marriageData$GP)
marriageData$GP[grepl("A", marriageData$GP, ignore.case = TRUE)] <- 0
marriageData$GP[grepl("B", marriageData$GP, ignore.case = TRUE)] <- 1
marriageData$GP <- as.numeric(marriageData$GP)


marriageData$BP <- as.character(marriageData$BP)
marriageData$BP[grepl("A", marriageData$BP, ignore.case = TRUE)] <- 0
marriageData$BP[grepl("B", marriageData$BP, ignore.case = TRUE)] <- 1
marriageData$BP <- as.numeric(marriageData$BP)

marriageData$STATUS <- as.character(marriageData$STATUS)
marriageData$STATUS[grepl("Married", marriageData$STATUS, ignore.case = TRUE)] <- 1
marriageData$STATUS[grepl("Separated", marriageData$STATUS, ignore.case = TRUE)] <- 2
marriageData$STATUS[grepl("Divorced", marriageData$STATUS, ignore.case = TRUE)] <- 3
marriageData$STATUS <- as.numeric(marriageData$STATUS)

marriageData$dAge <- 0

for(i in 1:nrow(marriageData)){
  marriageData$dAge[i] <- abs(marriageData$GAGE[i] - marriageData$BAGE[i])  
}

marriageData$GAGE <- NULL
marriageData$BAGE <- NULL

```

##Muestreo

Dado que se tienen  posibles valores (1-3) para el variable que se desea predecir "STATUS", se realiza un muestreo estratificado para asegurar que el mismo sea proporcional y exista un equilibrio en las muestras de entrenamiento y prueba.

```{r, warning= FALSE}
for (i in 1:3) {
  
  aux <- marriageData[marriageData$STATUS == i, ]
  muestraAux <- sample(nrow(aux), nrow(aux) * 0.8, replace = FALSE , prob=NULL)
  trainAux <- aux[muestraAux,]
  testAux <- aux[-muestraAux,]
  if(i == 1){
    training <- trainAux
    testing <- testAux
  }else{
    training <- merge(training, trainAux, all = TRUE)
    testing <- merge(testing, testAux, all = TRUE)
  }
}
```

A continuación se mostrará todo el procedimiento realizado para evaluar la calidad de los modelos basados en **Arboles de desición, k vecinos mas cercanos y reglas de clasificación**, entorno a la predicción del estado civil de una pareja.

##Arbol de desición 

```{r, warning= FALSE}
set.seed(3143)
tree <- rpart(STATUS ~ ., data = training , method ="class")
rpart.plot(tree)

predictArbol <- predict(tree, testing, type = "class") 

#MAtriz de confusión
matrizConfA<-table(testing$STATUS,predictArbol)  
matrizConfA

#Tasa de Aciertos
AciertosA <- sum(diag(matrizConfA)) / nrow(testing)
AciertosA

#ROC
rArbol <- roc(testing$STATUS, as.numeric(predictArbol), levels=c(1,2,3))
plot(rArbol)


```

##k-vecinos

```{r, warning= FALSE}
set.seed(5544)
#Se reacomodan los parámetros a ser pasados para el algoritmo de K-Vecinos, para esto se debió normalizar las columnas y eliminar las etiquetas de los conjuntos de entrenamiento (training) y prueba (training).

trainingLabels <- training$STATUS
testingLabels <- testing$STATUS

trainingClasificationRules <- training
testingClasificationRules <- testing
training[,"STATUS"] <- NULL
testing[,"STATUS"] <- NULL

predictKnn <- knn(train = training, test = testing, cl = trainingLabels, k=28) #test = testing 

#Matriz de confusión
matrizConfusionKnn <- table(testingLabels, predictKnn)
matrizConfusionKnn

#Tasa de Aciertos
AciertosKnn <- sum(diag(matrizConfusionKnn)) / nrow(testing)
AciertosKnn

#ROC
rKnn <- roc(testingLabels, as.numeric(predictKnn), levels=c(1,2,3))
plot(rKnn)

```

##Reglas de clasificación

```{r, warning= FALSE}
set.seed(8751)
#Se generó el modelo de Reglas de Clasificación haciendo uso de la función JRip provista por la interfaz RWeka, para ello debimos transformar los datos como se muestra a continuación:

trainingClasificationRules$STATUS <- as.factor(trainingClasificationRules$STATUS)
testingClasificationRules$STATUS <- as.factor(testingClasificationRules$STATUS)

#Aplicacion de la funcion JRip
rules <- JRip(formula = STATUS ~ ., data = trainingClasificationRules)

predictClass <- predict(rules, testingClasificationRules, type = "class")

# MAtriz de confusión
matrizConfusionClass <- table(testingClasificationRules$STATUS, predictClass) 
matrizConfusionClass

#Tasa de Aciertos
AciertosClass <-  sum(diag(matrizConfusionClass)) / nrow(testing)
AciertosClass

#ROC
rClass <- roc(testingLabels, as.numeric(predictClass), levels=c(1,2,3))
plot(rClass)

```

##¿Cuál de los tres modelos es mejor y por qué?

Para comparar los resultados obtenidos entre los tres modelos implementados, se tomó en cuenta la matriz de confusión, la tasa de aciertos y el area bajo la curva (función roc del paquete pROC) de cada uno de ellos. Se tiene que el mejor modelo obtenido es el basado en Reglas de Clasificación, la tasa de aciertos es perfecta al igual que el area bajo la curva, es decir, para ambos casos el resultado fue de 1. Seguidamente el modelo basado en árbol de desición con una tasa de aciertos de 0.91 y con un área bajo la curva de 0.97. Y por último se tiene el módelo de K-Vecinos el cual su tasa de aciertos fue de 0.5 y el area bajo la curva de 0.62 por lo que queda totalmente descartado como mejor modelo. Sin duda alguna el mejor modelo para el caso estudiado es el modelo basado en Reglas de Clasficación.

